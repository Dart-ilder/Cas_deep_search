{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sXz1K8g1Ki-1"
      },
      "source": [
        "# Embedings precomputing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependancies and mount GDrive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not running on CoLab\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Google Colab to run dumb-colab speciefic code\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    COLAB = True\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    COLAB=False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfAiyFfPUijQ",
        "outputId": "7a3f1f4c-d19d-4dcd-a655-df094a0dbd34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GPUtil in s:\\cas_deep_search\\.conda\\lib\\site-packages (1.4.0)\n",
            "Requirement already satisfied: pynvml in s:\\cas_deep_search\\.conda\\lib\\site-packages (11.5.0)\n",
            "Requirement already satisfied: ankh in s:\\cas_deep_search\\.conda\\lib\\site-packages (1.0.0)\n",
            "Requirement already satisfied: biopython<2.0,>=1.80 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from ankh) (1.81)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.7.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from ankh) (2.11.0)\n",
            "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.97 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from ankh) (0.1.98)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from ankh) (4.28.1)\n",
            "Requirement already satisfied: numpy in s:\\cas_deep_search\\.conda\\lib\\site-packages (from biopython<2.0,>=1.80->ankh) (1.24.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (6.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (2.29.0)\n",
            "Requirement already satisfied: xxhash in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (3.8.4)\n",
            "Requirement already satisfied: pandas in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (2.0.1)\n",
            "Requirement already satisfied: packaging in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (23.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (2023.3.23)\n",
            "Requirement already satisfied: filelock in s:\\cas_deep_search\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (0.13.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (2.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets<3.0.0,>=2.7.1->ankh) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (3.4)\n",
            "Requirement already satisfied: colorama in s:\\cas_deep_search\\.conda\\lib\\site-packages (from tqdm>=4.62.1->datasets<3.0.0,>=2.7.1->ankh) (0.4.6)\n",
            "Requirement already satisfied: tzdata>=2022.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.7.1->ankh) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.7.1->ankh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.7.1->ankh) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in s:\\cas_deep_search\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets<3.0.0,>=2.7.1->ankh) (1.16.0)\n",
            "Requirement already satisfied: tqdm in s:\\cas_deep_search\\.conda\\lib\\site-packages (4.65.0)\n",
            "Requirement already satisfied: colorama in s:\\cas_deep_search\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# Other than torch dependencies\n",
        "# ! pip install fair-esm # Switched to Ankh embeddings\n",
        "! pip install GPUtil\n",
        "! pip install pynvml\n",
        "! pip install ankh\n",
        "! pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mWozLpHAe1Zv",
        "outputId": "a6117dd0-8dab-47b9-d551-5e0c3cacfaa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pytorch 1.13.1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Pytorch \" + torch.__version__)\n",
        "# import esm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "\n",
        "import ankh\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn import metrics\n",
        "from scipy import stats\n",
        "from functools import partial\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Create device agnostic code\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hxkZ0mZ-KrSA"
      },
      "source": [
        "### Load data from TSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMLTUMt_I5FK",
        "outputId": "1a22fa46-1b8c-43a5-b4c4-ebf4628c2240"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "    # Mount GDrive for Colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Navigate Colab\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/esm\n",
        "    %ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Loci_id</th>\n",
              "      <th>Strand</th>\n",
              "      <th>Genome_assembly</th>\n",
              "      <th>Chromosome</th>\n",
              "      <th>N1</th>\n",
              "      <th>Protein_id</th>\n",
              "      <th>B2</th>\n",
              "      <th>Gene_id</th>\n",
              "      <th>Gene_family</th>\n",
              "      <th>Type</th>\n",
              "      <th>Species</th>\n",
              "      <th>Start</th>\n",
              "      <th>End</th>\n",
              "      <th>Seq</th>\n",
              "      <th>Prot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RN99_05230</td>\n",
              "      <td>-</td>\n",
              "      <td>GCA_001296125.1</td>\n",
              "      <td>CP012714.1</td>\n",
              "      <td>991.0</td>\n",
              "      <td>ALF19893_1</td>\n",
              "      <td>+</td>\n",
              "      <td>cd09644</td>\n",
              "      <td>csn2</td>\n",
              "      <td>CAS-II-A</td>\n",
              "      <td>Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...</td>\n",
              "      <td>1100111</td>\n",
              "      <td>1100773</td>\n",
              "      <td>TTAGAAAATTTCACATAAATCATTATCTATAACAATTAAATTATCA...</td>\n",
              "      <td>MTFQYKGFNFKIDFEEKNIFSLIVENKRAYRKIIEDLVNNSNIEDG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RN99_05235</td>\n",
              "      <td>-</td>\n",
              "      <td>GCA_001296125.1</td>\n",
              "      <td>CP012714.1</td>\n",
              "      <td>992.0</td>\n",
              "      <td>ALF20727_1</td>\n",
              "      <td>+</td>\n",
              "      <td>mkCas0206</td>\n",
              "      <td>cas2</td>\n",
              "      <td>CAS-II-A</td>\n",
              "      <td>Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...</td>\n",
              "      <td>1100770</td>\n",
              "      <td>1101075</td>\n",
              "      <td>TCATAAAACCACAAGCCTTTCATCTGTTTCTAAAAATGTCCCTTTT...</td>\n",
              "      <td>MRMLLFFDLPSVTNSDLKEYRKFRKFLIENGFSMLQESVYSKLLLH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RN99_05240</td>\n",
              "      <td>-</td>\n",
              "      <td>GCA_001296125.1</td>\n",
              "      <td>CP012714.1</td>\n",
              "      <td>993.0</td>\n",
              "      <td>ALF19894_1</td>\n",
              "      <td>+</td>\n",
              "      <td>cd09720</td>\n",
              "      <td>cas1</td>\n",
              "      <td>CAS-II-A</td>\n",
              "      <td>Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...</td>\n",
              "      <td>1101080</td>\n",
              "      <td>1101958</td>\n",
              "      <td>CTATAACTCATCTTGAAAAAATCTCACTAATGATAAATCATTTGAG...</td>\n",
              "      <td>MSGWRVIIVTGRGKLDLRYNSISIRRDNGTDFIHIGEVNTLILETT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Loci_id Strand  Genome_assembly  Chromosome     N1  Protein_id B2  \\\n",
              "0  RN99_05230      -  GCA_001296125.1  CP012714.1  991.0  ALF19893_1  +   \n",
              "1  RN99_05235      -  GCA_001296125.1  CP012714.1  992.0  ALF20727_1  +   \n",
              "2  RN99_05240      -  GCA_001296125.1  CP012714.1  993.0  ALF19894_1  +   \n",
              "\n",
              "     Gene_id Gene_family      Type  \\\n",
              "0    cd09644        csn2  CAS-II-A   \n",
              "1  mkCas0206        cas2  CAS-II-A   \n",
              "2    cd09720        cas1  CAS-II-A   \n",
              "\n",
              "                                             Species    Start      End  \\\n",
              "0  Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...  1100111  1100773   \n",
              "1  Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...  1100770  1101075   \n",
              "2  Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...  1101080  1101958   \n",
              "\n",
              "                                                 Seq  \\\n",
              "0  TTAGAAAATTTCACATAAATCATTATCTATAACAATTAAATTATCA...   \n",
              "1  TCATAAAACCACAAGCCTTTCATCTGTTTCTAAAAATGTCCCTTTT...   \n",
              "2  CTATAACTCATCTTGAAAAAATCTCACTAATGATAAATCATTTGAG...   \n",
              "\n",
              "                                                Prot  \n",
              "0  MTFQYKGFNFKIDFEEKNIFSLIVENKRAYRKIIEDLVNNSNIEDG...  \n",
              "1  MRMLLFFDLPSVTNSDLKEYRKFRKFLIENGFSMLQESVYSKLLLH...  \n",
              "2  MSGWRVIIVTGRGKLDLRYNSISIRRDNGTDFIHIGEVNTLILETT...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download dataset ~56MB\n",
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Dart-ilder/Cas_deep_search/main/esm/emb_pregen/cas_dataset_kira.tsv'\n",
        "response = requests.get(url)\n",
        "\n",
        "with open('./cas_dataset_kira.tsv', 'w') as file:\n",
        "    file.write(response.text)\n",
        "\n",
        "with open('./cas_dataset_kira.tsv') as file:\n",
        "    cas_voc = pd.read_csv(file, delimiter=\"\\t\", comment='=')\n",
        "cas_voc.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ElnaggarLab/ankh-base were not used when initializing T5EncoderModel: ['decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'lm_head.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight']\n",
            "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "T5EncoderModel(\n",
              "  (shared): Embedding(144, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(144, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(64, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-47): 47 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model, tokenizer = ankh.load_base_model()\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Loci_id</th>\n",
              "      <th>Strand</th>\n",
              "      <th>Genome_assembly</th>\n",
              "      <th>Chromosome</th>\n",
              "      <th>N1</th>\n",
              "      <th>Protein_id</th>\n",
              "      <th>B2</th>\n",
              "      <th>Gene_id</th>\n",
              "      <th>Gene_family</th>\n",
              "      <th>Type</th>\n",
              "      <th>Species</th>\n",
              "      <th>Start</th>\n",
              "      <th>End</th>\n",
              "      <th>Seq</th>\n",
              "      <th>Prot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RN99_05230</td>\n",
              "      <td>-</td>\n",
              "      <td>GCA_001296125.1</td>\n",
              "      <td>CP012714.1</td>\n",
              "      <td>991.0</td>\n",
              "      <td>ALF19893_1</td>\n",
              "      <td>+</td>\n",
              "      <td>cd09644</td>\n",
              "      <td>csn2</td>\n",
              "      <td>CAS-II-A</td>\n",
              "      <td>Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...</td>\n",
              "      <td>1100111</td>\n",
              "      <td>1100773</td>\n",
              "      <td>TTAGAAAATTTCACATAAATCATTATCTATAACAATTAAATTATCA...</td>\n",
              "      <td>MTFQYKGFNFKIDFEEKNIFSLIVENKRAYRKIIEDLVNNSNIEDG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RN99_05235</td>\n",
              "      <td>-</td>\n",
              "      <td>GCA_001296125.1</td>\n",
              "      <td>CP012714.1</td>\n",
              "      <td>992.0</td>\n",
              "      <td>ALF20727_1</td>\n",
              "      <td>+</td>\n",
              "      <td>mkCas0206</td>\n",
              "      <td>cas2</td>\n",
              "      <td>CAS-II-A</td>\n",
              "      <td>Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...</td>\n",
              "      <td>1100770</td>\n",
              "      <td>1101075</td>\n",
              "      <td>TCATAAAACCACAAGCCTTTCATCTGTTTCTAAAAATGTCCCTTTT...</td>\n",
              "      <td>MRMLLFFDLPSVTNSDLKEYRKFRKFLIENGFSMLQESVYSKLLLH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RN99_05240</td>\n",
              "      <td>-</td>\n",
              "      <td>GCA_001296125.1</td>\n",
              "      <td>CP012714.1</td>\n",
              "      <td>993.0</td>\n",
              "      <td>ALF19894_1</td>\n",
              "      <td>+</td>\n",
              "      <td>cd09720</td>\n",
              "      <td>cas1</td>\n",
              "      <td>CAS-II-A</td>\n",
              "      <td>Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...</td>\n",
              "      <td>1101080</td>\n",
              "      <td>1101958</td>\n",
              "      <td>CTATAACTCATCTTGAAAAAATCTCACTAATGATAAATCATTTGAG...</td>\n",
              "      <td>MSGWRVIIVTGRGKLDLRYNSISIRRDNGTDFIHIGEVNTLILETT...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Loci_id Strand  Genome_assembly  Chromosome     N1  Protein_id B2   \n",
              "0  RN99_05230      -  GCA_001296125.1  CP012714.1  991.0  ALF19893_1  +  \\\n",
              "1  RN99_05235      -  GCA_001296125.1  CP012714.1  992.0  ALF20727_1  +   \n",
              "2  RN99_05240      -  GCA_001296125.1  CP012714.1  993.0  ALF19894_1  +   \n",
              "\n",
              "     Gene_id Gene_family      Type   \n",
              "0    cd09644        csn2  CAS-II-A  \\\n",
              "1  mkCas0206        cas2  CAS-II-A   \n",
              "2    cd09720        cas1  CAS-II-A   \n",
              "\n",
              "                                             Species    Start      End   \n",
              "0  Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...  1100111  1100773  \\\n",
              "1  Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...  1100770  1101075   \n",
              "2  Fusobacterium_nucleatum_sub_vincentii_ChDC_F8_...  1101080  1101958   \n",
              "\n",
              "                                                 Seq   \n",
              "0  TTAGAAAATTTCACATAAATCATTATCTATAACAATTAAATTATCA...  \\\n",
              "1  TCATAAAACCACAAGCCTTTCATCTGTTTCTAAAAATGTCCCTTTT...   \n",
              "2  CTATAACTCATCTTGAAAAAATCTCACTAATGATAAATCATTTGAG...   \n",
              "\n",
              "                                                Prot  \n",
              "0  MTFQYKGFNFKIDFEEKNIFSLIVENKRAYRKIIEDLVNNSNIEDG...  \n",
              "1  MRMLLFFDLPSVTNSDLKEYRKFRKFLIENGFSMLQESVYSKLLLH...  \n",
              "2  MSGWRVIIVTGRGKLDLRYNSISIRRDNGTDFIHIGEVNTLILETT...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cas_voc.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cache_clear(): # In colab to clear GPU cache you need to wait some time after deleting tensor\n",
        "    if COLAB:\n",
        "        time.sleep(0.02)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "def gpu_util(): # To monitor how much more can we load GPU with data\n",
        "    if DEVICE == \"cuda\":\n",
        "        return torch.cuda.memory_reserved(DEVICE)/torch.cuda.get_device_properties(DEVICE).total_memory\n",
        "    if DEVICE == \"cpu\":\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([36970, 1811])\n"
          ]
        }
      ],
      "source": [
        "protein_sequences = list(cas_voc[\"Prot\"].values)\n",
        "tokens = tokenizer.batch_encode_plus(protein_sequences, \n",
        "                                        add_special_tokens=True, \n",
        "                                        padding=True,\n",
        "                                        is_split_into_words=False, \n",
        "                                        return_tensors=\"pt\")\n",
        "print(tokens['input_ids'].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2015363072\n",
            "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1080 Ti', major=6, minor=1, total_memory=11263MB, multi_processor_count=28)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.17063588946842748"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cache_clear()\n",
        "print(torch.cuda.memory_reserved(DEVICE))\n",
        "print(torch.cuda.get_device_properties(DEVICE))\n",
        "import gc\n",
        "gc.collect()\n",
        "gpu_util()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([15, 1811])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SIZE = 15 # For GTX 1080ti\n",
        "batched_tokens = torch.split(tokens['input_ids'], BATCH_SIZE)\n",
        "batched_attention_masks = torch.split(tokens['attention_mask'], BATCH_SIZE)\n",
        "len(batched_tokens)\n",
        "batched_tokens[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 26% | 23% |\n",
            "0/2465\t GPU Usage:0.8382643435800688\n",
            "1/2465\t GPU Usage:0.8382643435800688\n",
            "2/2465\t GPU Usage:0.8382643435800688\n",
            "3/2465\t GPU Usage:0.8382643435800688\n",
            "4/2465\t GPU Usage:0.8382643435800688\n",
            "5/2465\t GPU Usage:0.8382643435800688\n",
            "6/2465\t GPU Usage:0.8382643435800688\n",
            "7/2465\t GPU Usage:0.8382643435800688\n",
            "8/2465\t GPU Usage:0.8382643435800688\n",
            "9/2465\t GPU Usage:0.8382643435800688\n",
            "10/2465\t GPU Usage:0.8382643435800688\n",
            "11/2465\t GPU Usage:0.8382643435800688\n",
            "12/2465\t GPU Usage:0.8382643435800688\n",
            "13/2465\t GPU Usage:0.8382643435800688\n",
            "14/2465\t GPU Usage:0.8382643435800688\n",
            "15/2465\t GPU Usage:0.8382643435800688\n",
            "16/2465\t GPU Usage:0.8382643435800688\n",
            "17/2465\t GPU Usage:0.8382643435800688\n",
            "18/2465\t GPU Usage:0.8382643435800688\n",
            "19/2465\t GPU Usage:0.8382643435800688\n",
            "20/2465\t GPU Usage:0.8382643435800688\n",
            "21/2465\t GPU Usage:0.8382643435800688\n",
            "22/2465\t GPU Usage:0.8382643435800688\n",
            "23/2465\t GPU Usage:0.8382643435800688\n",
            "24/2465\t GPU Usage:0.8382643435800688\n",
            "25/2465\t GPU Usage:0.8382643435800688\n",
            "26/2465\t GPU Usage:0.8382643435800688\n",
            "27/2465\t GPU Usage:0.8382643435800688\n",
            "28/2465\t GPU Usage:0.8382643435800688\n",
            "29/2465\t GPU Usage:0.8382643435800688\n",
            "30/2465\t GPU Usage:0.8382643435800688\n",
            "31/2465\t GPU Usage:0.8382643435800688\n",
            "32/2465\t GPU Usage:0.8382643435800688\n",
            "33/2465\t GPU Usage:0.8382643435800688\n",
            "34/2465\t GPU Usage:0.8382643435800688\n",
            "35/2465\t GPU Usage:0.8382643435800688\n",
            "36/2465\t GPU Usage:0.8382643435800688\n",
            "37/2465\t GPU Usage:0.8382643435800688\n",
            "38/2465\t GPU Usage:0.8382643435800688\n",
            "39/2465\t GPU Usage:0.8382643435800688\n",
            "40/2465\t GPU Usage:0.8382643435800688\n",
            "41/2465\t GPU Usage:0.8382643435800688\n",
            "42/2465\t GPU Usage:0.8382643435800688\n",
            "43/2465\t GPU Usage:0.8382643435800688\n",
            "44/2465\t GPU Usage:0.8382643435800688\n",
            "45/2465\t GPU Usage:0.8382643435800688\n",
            "46/2465\t GPU Usage:0.8382643435800688\n",
            "47/2465\t GPU Usage:0.8382643435800688\n",
            "48/2465\t GPU Usage:0.8382643435800688\n",
            "49/2465\t GPU Usage:0.8382643435800688\n",
            "50/2465\t GPU Usage:0.8382643435800688\n",
            "51/2465\t GPU Usage:0.8382643435800688\n",
            "52/2465\t GPU Usage:0.8382643435800688\n",
            "53/2465\t GPU Usage:0.8382643435800688\n",
            "54/2465\t GPU Usage:0.8382643435800688\n",
            "55/2465\t GPU Usage:0.8382643435800688\n",
            "56/2465\t GPU Usage:0.8382643435800688\n",
            "57/2465\t GPU Usage:0.8382643435800688\n",
            "58/2465\t GPU Usage:0.8382643435800688\n",
            "59/2465\t GPU Usage:0.8382643435800688\n",
            "60/2465\t GPU Usage:0.8382643435800688\n",
            "61/2465\t GPU Usage:0.8382643435800688\n",
            "62/2465\t GPU Usage:0.8382643435800688\n",
            "63/2465\t GPU Usage:0.8382643435800688\n",
            "64/2465\t GPU Usage:0.8382643435800688\n",
            "65/2465\t GPU Usage:0.8382643435800688\n",
            "66/2465\t GPU Usage:0.8382643435800688\n",
            "67/2465\t GPU Usage:0.8382643435800688\n",
            "68/2465\t GPU Usage:0.8382643435800688\n",
            "69/2465\t GPU Usage:0.8382643435800688\n",
            "70/2465\t GPU Usage:0.8382643435800688\n",
            "71/2465\t GPU Usage:0.8382643435800688\n",
            "72/2465\t GPU Usage:0.8382643435800688\n",
            "73/2465\t GPU Usage:0.8382643435800688\n",
            "74/2465\t GPU Usage:0.8382643435800688\n",
            "75/2465\t GPU Usage:0.8382643435800688\n",
            "76/2465\t GPU Usage:0.8382643435800688\n",
            "77/2465\t GPU Usage:0.8382643435800688\n",
            "78/2465\t GPU Usage:0.8382643435800688\n",
            "79/2465\t GPU Usage:0.8382643435800688\n",
            "80/2465\t GPU Usage:0.8382643435800688\n",
            "81/2465\t GPU Usage:0.8382643435800688\n",
            "82/2465\t GPU Usage:0.8382643435800688\n",
            "83/2465\t GPU Usage:0.8382643435800688\n",
            "84/2465\t GPU Usage:0.8382643435800688\n",
            "85/2465\t GPU Usage:0.8382643435800688\n",
            "86/2465\t GPU Usage:0.8382643435800688\n",
            "87/2465\t GPU Usage:0.8382643435800688\n",
            "88/2465\t GPU Usage:0.8382643435800688\n",
            "89/2465\t GPU Usage:0.8382643435800688\n",
            "90/2465\t GPU Usage:0.8382643435800688\n",
            "91/2465\t GPU Usage:0.8382643435800688\n",
            "92/2465\t GPU Usage:0.8382643435800688\n",
            "93/2465\t GPU Usage:0.8382643435800688\n",
            "94/2465\t GPU Usage:0.8382643435800688\n",
            "95/2465\t GPU Usage:0.8382643435800688\n",
            "96/2465\t GPU Usage:0.8382643435800688\n",
            "97/2465\t GPU Usage:0.8382643435800688\n",
            "98/2465\t GPU Usage:0.8382643435800688\n",
            "99/2465\t GPU Usage:0.8382643435800688\n",
            "100/2465\t GPU Usage:0.8382643435800688\n",
            "101/2465\t GPU Usage:0.8382643435800688\n",
            "102/2465\t GPU Usage:0.8382643435800688\n",
            "103/2465\t GPU Usage:0.8382643435800688\n",
            "104/2465\t GPU Usage:0.8382643435800688\n",
            "105/2465\t GPU Usage:0.8382643435800688\n",
            "106/2465\t GPU Usage:0.8382643435800688\n",
            "107/2465\t GPU Usage:0.8382643435800688\n",
            "108/2465\t GPU Usage:0.8382643435800688\n",
            "109/2465\t GPU Usage:0.8382643435800688\n",
            "110/2465\t GPU Usage:0.8382643435800688\n",
            "111/2465\t GPU Usage:0.8382643435800688\n",
            "112/2465\t GPU Usage:0.8382643435800688\n",
            "113/2465\t GPU Usage:0.8382643435800688\n",
            "114/2465\t GPU Usage:0.8382643435800688\n",
            "115/2465\t GPU Usage:0.8382643435800688\n",
            "116/2465\t GPU Usage:0.8382643435800688\n",
            "117/2465\t GPU Usage:0.8382643435800688\n",
            "118/2465\t GPU Usage:0.8382643435800688\n",
            "119/2465\t GPU Usage:0.8382643435800688\n",
            "120/2465\t GPU Usage:0.8382643435800688\n",
            "121/2465\t GPU Usage:0.8382643435800688\n",
            "122/2465\t GPU Usage:0.8382643435800688\n",
            "123/2465\t GPU Usage:0.8382643435800688\n",
            "124/2465\t GPU Usage:0.8382643435800688\n",
            "125/2465\t GPU Usage:0.8382643435800688\n",
            "126/2465\t GPU Usage:0.8382643435800688\n",
            "127/2465\t GPU Usage:0.8382643435800688\n",
            "128/2465\t GPU Usage:0.8382643435800688\n",
            "129/2465\t GPU Usage:0.8382643435800688\n",
            "130/2465\t GPU Usage:0.8382643435800688\n",
            "131/2465\t GPU Usage:0.8382643435800688\n",
            "132/2465\t GPU Usage:0.8382643435800688\n",
            "133/2465\t GPU Usage:0.8382643435800688\n",
            "134/2465\t GPU Usage:0.8382643435800688\n",
            "135/2465\t GPU Usage:0.8382643435800688\n",
            "136/2465\t GPU Usage:0.8382643435800688\n",
            "137/2465\t GPU Usage:0.8382643435800688\n",
            "138/2465\t GPU Usage:0.8382643435800688\n",
            "139/2465\t GPU Usage:0.8382643435800688\n",
            "140/2465\t GPU Usage:0.8382643435800688\n",
            "141/2465\t GPU Usage:0.8382643435800688\n",
            "142/2465\t GPU Usage:0.8382643435800688\n",
            "143/2465\t GPU Usage:0.8382643435800688\n",
            "144/2465\t GPU Usage:0.8382643435800688\n",
            "145/2465\t GPU Usage:0.8382643435800688\n",
            "146/2465\t GPU Usage:0.8382643435800688\n",
            "147/2465\t GPU Usage:0.8382643435800688\n",
            "148/2465\t GPU Usage:0.8382643435800688\n",
            "149/2465\t GPU Usage:0.8382643435800688\n",
            "150/2465\t GPU Usage:0.8382643435800688\n",
            "151/2465\t GPU Usage:0.8382643435800688\n",
            "152/2465\t GPU Usage:0.8382643435800688\n",
            "153/2465\t GPU Usage:0.8382643435800688\n",
            "154/2465\t GPU Usage:0.8382643435800688\n",
            "155/2465\t GPU Usage:0.8382643435800688\n",
            "156/2465\t GPU Usage:0.8382643435800688\n",
            "157/2465\t GPU Usage:0.8382643435800688\n",
            "158/2465\t GPU Usage:0.8382643435800688\n",
            "159/2465\t GPU Usage:0.8382643435800688\n",
            "160/2465\t GPU Usage:0.8382643435800688\n",
            "161/2465\t GPU Usage:0.8382643435800688\n",
            "162/2465\t GPU Usage:0.8382643435800688\n",
            "163/2465\t GPU Usage:0.8382643435800688\n",
            "164/2465\t GPU Usage:0.8382643435800688\n",
            "165/2465\t GPU Usage:0.8382643435800688\n",
            "166/2465\t GPU Usage:0.8382643435800688\n",
            "167/2465\t GPU Usage:0.8382643435800688\n",
            "168/2465\t GPU Usage:0.8382643435800688\n",
            "169/2465\t GPU Usage:0.8382643435800688\n",
            "170/2465\t GPU Usage:0.8382643435800688\n",
            "171/2465\t GPU Usage:0.8382643435800688\n",
            "172/2465\t GPU Usage:0.8382643435800688\n",
            "173/2465\t GPU Usage:0.8382643435800688\n",
            "174/2465\t GPU Usage:0.8382643435800688\n",
            "175/2465\t GPU Usage:0.8382643435800688\n",
            "176/2465\t GPU Usage:0.8382643435800688\n",
            "177/2465\t GPU Usage:0.8382643435800688\n",
            "178/2465\t GPU Usage:0.8382643435800688\n",
            "179/2465\t GPU Usage:0.8382643435800688\n",
            "180/2465\t GPU Usage:0.8382643435800688\n",
            "181/2465\t GPU Usage:0.8382643435800688\n",
            "182/2465\t GPU Usage:0.8382643435800688\n",
            "183/2465\t GPU Usage:0.8382643435800688\n",
            "184/2465\t GPU Usage:0.8382643435800688\n",
            "185/2465\t GPU Usage:0.8382643435800688\n",
            "186/2465\t GPU Usage:0.8382643435800688\n",
            "187/2465\t GPU Usage:0.8382643435800688\n",
            "188/2465\t GPU Usage:0.8382643435800688\n",
            "189/2465\t GPU Usage:0.8382643435800688\n",
            "190/2465\t GPU Usage:0.8382643435800688\n",
            "191/2465\t GPU Usage:0.8382643435800688\n",
            "192/2465\t GPU Usage:0.8382643435800688\n",
            "193/2465\t GPU Usage:0.8382643435800688\n",
            "194/2465\t GPU Usage:0.8382643435800688\n",
            "195/2465\t GPU Usage:0.8382643435800688\n",
            "196/2465\t GPU Usage:0.8382643435800688\n",
            "197/2465\t GPU Usage:0.8382643435800688\n",
            "198/2465\t GPU Usage:0.8382643435800688\n",
            "199/2465\t GPU Usage:0.8382643435800688\n",
            "200/2465\t GPU Usage:0.8382643435800688\n",
            "201/2465\t GPU Usage:0.8382643435800688\n",
            "202/2465\t GPU Usage:0.8382643435800688\n",
            "203/2465\t GPU Usage:0.8382643435800688\n",
            "204/2465\t GPU Usage:0.8382643435800688\n",
            "205/2465\t GPU Usage:0.8382643435800688\n",
            "206/2465\t GPU Usage:0.8382643435800688\n",
            "207/2465\t GPU Usage:0.8382643435800688\n",
            "208/2465\t GPU Usage:0.8382643435800688\n",
            "209/2465\t GPU Usage:0.8382643435800688\n",
            "210/2465\t GPU Usage:0.8382643435800688\n",
            "211/2465\t GPU Usage:0.8382643435800688\n",
            "212/2465\t GPU Usage:0.8382643435800688\n",
            "213/2465\t GPU Usage:0.8382643435800688\n",
            "214/2465\t GPU Usage:0.8382643435800688\n",
            "215/2465\t GPU Usage:0.8382643435800688\n",
            "216/2465\t GPU Usage:0.8382643435800688\n",
            "217/2465\t GPU Usage:0.8382643435800688\n",
            "218/2465\t GPU Usage:0.8382643435800688\n",
            "219/2465\t GPU Usage:0.8382643435800688\n",
            "220/2465\t GPU Usage:0.8382643435800688\n",
            "221/2465\t GPU Usage:0.8382643435800688\n",
            "222/2465\t GPU Usage:0.8382643435800688\n",
            "223/2465\t GPU Usage:0.8382643435800688\n",
            "224/2465\t GPU Usage:0.8382643435800688\n",
            "225/2465\t GPU Usage:0.8382643435800688\n",
            "226/2465\t GPU Usage:0.8382643435800688\n",
            "227/2465\t GPU Usage:0.8382643435800688\n",
            "228/2465\t GPU Usage:0.8382643435800688\n",
            "229/2465\t GPU Usage:0.8382643435800688\n",
            "230/2465\t GPU Usage:0.8382643435800688\n",
            "231/2465\t GPU Usage:0.8382643435800688\n",
            "232/2465\t GPU Usage:0.8382643435800688\n",
            "233/2465\t GPU Usage:0.8382643435800688\n",
            "234/2465\t GPU Usage:0.8382643435800688\n",
            "235/2465\t GPU Usage:0.8382643435800688\n",
            "236/2465\t GPU Usage:0.8382643435800688\n",
            "237/2465\t GPU Usage:0.8382643435800688\n",
            "238/2465\t GPU Usage:0.8382643435800688\n",
            "239/2465\t GPU Usage:0.8382643435800688\n",
            "240/2465\t GPU Usage:0.8382643435800688\n",
            "241/2465\t GPU Usage:0.8382643435800688\n",
            "242/2465\t GPU Usage:0.8382643435800688\n",
            "243/2465\t GPU Usage:0.8382643435800688\n",
            "244/2465\t GPU Usage:0.8382643435800688\n",
            "245/2465\t GPU Usage:0.8382643435800688\n",
            "246/2465\t GPU Usage:0.8382643435800688\n",
            "247/2465\t GPU Usage:0.8382643435800688\n",
            "248/2465\t GPU Usage:0.8382643435800688\n",
            "249/2465\t GPU Usage:0.8382643435800688\n",
            "250/2465\t GPU Usage:0.8382643435800688\n",
            "251/2465\t GPU Usage:0.8382643435800688\n",
            "252/2465\t GPU Usage:0.8382643435800688\n",
            "253/2465\t GPU Usage:0.8382643435800688\n",
            "254/2465\t GPU Usage:0.8382643435800688\n",
            "255/2465\t GPU Usage:0.8382643435800688\n",
            "256/2465\t GPU Usage:0.8382643435800688\n",
            "257/2465\t GPU Usage:0.8382643435800688\n",
            "258/2465\t GPU Usage:0.8382643435800688\n",
            "259/2465\t GPU Usage:0.8382643435800688\n",
            "260/2465\t GPU Usage:0.8382643435800688\n",
            "261/2465\t GPU Usage:0.8382643435800688\n",
            "262/2465\t GPU Usage:0.8382643435800688\n",
            "263/2465\t GPU Usage:0.8382643435800688\n",
            "264/2465\t GPU Usage:0.8382643435800688\n",
            "265/2465\t GPU Usage:0.8382643435800688\n",
            "266/2465\t GPU Usage:0.8382643435800688\n",
            "267/2465\t GPU Usage:0.8382643435800688\n",
            "268/2465\t GPU Usage:0.8382643435800688\n",
            "269/2465\t GPU Usage:0.8382643435800688\n",
            "270/2465\t GPU Usage:0.8382643435800688\n",
            "271/2465\t GPU Usage:0.8382643435800688\n",
            "272/2465\t GPU Usage:0.8382643435800688\n",
            "273/2465\t GPU Usage:0.8382643435800688\n",
            "274/2465\t GPU Usage:0.8382643435800688\n",
            "275/2465\t GPU Usage:0.8382643435800688\n",
            "276/2465\t GPU Usage:0.8382643435800688\n",
            "277/2465\t GPU Usage:0.8382643435800688\n",
            "278/2465\t GPU Usage:0.8382643435800688\n",
            "279/2465\t GPU Usage:0.8382643435800688\n",
            "280/2465\t GPU Usage:0.8382643435800688\n",
            "281/2465\t GPU Usage:0.8382643435800688\n",
            "282/2465\t GPU Usage:0.8382643435800688\n",
            "283/2465\t GPU Usage:0.8382643435800688\n",
            "284/2465\t GPU Usage:0.8382643435800688\n",
            "285/2465\t GPU Usage:0.8382643435800688\n",
            "286/2465\t GPU Usage:0.8382643435800688\n",
            "287/2465\t GPU Usage:0.8382643435800688\n",
            "288/2465\t GPU Usage:0.8382643435800688\n",
            "289/2465\t GPU Usage:0.8382643435800688\n",
            "290/2465\t GPU Usage:0.8382643435800688\n",
            "291/2465\t GPU Usage:0.8382643435800688\n",
            "292/2465\t GPU Usage:0.8382643435800688\n",
            "293/2465\t GPU Usage:0.8382643435800688\n",
            "294/2465\t GPU Usage:0.8382643435800688\n",
            "295/2465\t GPU Usage:0.8382643435800688\n",
            "296/2465\t GPU Usage:0.8382643435800688\n",
            "297/2465\t GPU Usage:0.8382643435800688\n",
            "298/2465\t GPU Usage:0.8382643435800688\n",
            "299/2465\t GPU Usage:0.8382643435800688\n",
            "300/2465\t GPU Usage:0.8382643435800688\n",
            "301/2465\t GPU Usage:0.8382643435800688\n",
            "302/2465\t GPU Usage:0.8382643435800688\n",
            "303/2465\t GPU Usage:0.8382643435800688\n",
            "304/2465\t GPU Usage:0.8382643435800688\n",
            "305/2465\t GPU Usage:0.8382643435800688\n",
            "306/2465\t GPU Usage:0.8382643435800688\n",
            "307/2465\t GPU Usage:0.8382643435800688\n",
            "308/2465\t GPU Usage:0.8382643435800688\n",
            "309/2465\t GPU Usage:0.8382643435800688\n",
            "310/2465\t GPU Usage:0.8382643435800688\n",
            "311/2465\t GPU Usage:0.8382643435800688\n",
            "312/2465\t GPU Usage:0.8382643435800688\n",
            "313/2465\t GPU Usage:0.8382643435800688\n",
            "314/2465\t GPU Usage:0.8382643435800688\n",
            "315/2465\t GPU Usage:0.8382643435800688\n",
            "316/2465\t GPU Usage:0.8382643435800688\n",
            "317/2465\t GPU Usage:0.8382643435800688\n",
            "318/2465\t GPU Usage:0.8382643435800688\n",
            "319/2465\t GPU Usage:0.8382643435800688\n",
            "320/2465\t GPU Usage:0.8382643435800688\n",
            "321/2465\t GPU Usage:0.8382643435800688\n",
            "322/2465\t GPU Usage:0.8382643435800688\n",
            "323/2465\t GPU Usage:0.8382643435800688\n",
            "324/2465\t GPU Usage:0.8382643435800688\n",
            "325/2465\t GPU Usage:0.8382643435800688\n",
            "326/2465\t GPU Usage:0.8382643435800688\n",
            "327/2465\t GPU Usage:0.8382643435800688\n",
            "328/2465\t GPU Usage:0.8382643435800688\n",
            "329/2465\t GPU Usage:0.8382643435800688\n",
            "330/2465\t GPU Usage:0.8382643435800688\n",
            "331/2465\t GPU Usage:0.8382643435800688\n",
            "332/2465\t GPU Usage:0.8382643435800688\n",
            "333/2465\t GPU Usage:0.8382643435800688\n",
            "334/2465\t GPU Usage:0.8382643435800688\n",
            "335/2465\t GPU Usage:0.8382643435800688\n",
            "336/2465\t GPU Usage:0.8382643435800688\n",
            "337/2465\t GPU Usage:0.8382643435800688\n",
            "338/2465\t GPU Usage:0.8382643435800688\n",
            "339/2465\t GPU Usage:0.8382643435800688\n",
            "340/2465\t GPU Usage:0.8382643435800688\n",
            "341/2465\t GPU Usage:0.8382643435800688\n",
            "342/2465\t GPU Usage:0.8382643435800688\n",
            "343/2465\t GPU Usage:0.8382643435800688\n",
            "344/2465\t GPU Usage:0.8382643435800688\n",
            "345/2465\t GPU Usage:0.8382643435800688\n",
            "346/2465\t GPU Usage:0.8382643435800688\n",
            "347/2465\t GPU Usage:0.8382643435800688\n",
            "348/2465\t GPU Usage:0.8382643435800688\n",
            "349/2465\t GPU Usage:0.8382643435800688\n",
            "350/2465\t GPU Usage:0.8382643435800688\n",
            "351/2465\t GPU Usage:0.8382643435800688\n",
            "352/2465\t GPU Usage:0.8382643435800688\n",
            "353/2465\t GPU Usage:0.8382643435800688\n",
            "354/2465\t GPU Usage:0.8382643435800688\n",
            "355/2465\t GPU Usage:0.8382643435800688\n",
            "356/2465\t GPU Usage:0.8382643435800688\n",
            "357/2465\t GPU Usage:0.8382643435800688\n",
            "358/2465\t GPU Usage:0.8382643435800688\n",
            "359/2465\t GPU Usage:0.8382643435800688\n",
            "360/2465\t GPU Usage:0.8382643435800688\n",
            "361/2465\t GPU Usage:0.8382643435800688\n",
            "362/2465\t GPU Usage:0.8382643435800688\n",
            "363/2465\t GPU Usage:0.8382643435800688\n",
            "364/2465\t GPU Usage:0.8382643435800688\n",
            "365/2465\t GPU Usage:0.8382643435800688\n",
            "366/2465\t GPU Usage:0.8382643435800688\n",
            "367/2465\t GPU Usage:0.8382643435800688\n",
            "368/2465\t GPU Usage:0.8382643435800688\n",
            "369/2465\t GPU Usage:0.8382643435800688\n",
            "370/2465\t GPU Usage:0.8382643435800688\n",
            "371/2465\t GPU Usage:0.8382643435800688\n",
            "372/2465\t GPU Usage:0.8382643435800688\n",
            "373/2465\t GPU Usage:0.8382643435800688\n",
            "374/2465\t GPU Usage:0.8382643435800688\n",
            "375/2465\t GPU Usage:0.8382643435800688\n",
            "376/2465\t GPU Usage:0.8382643435800688\n",
            "377/2465\t GPU Usage:0.8382643435800688\n",
            "378/2465\t GPU Usage:0.8382643435800688\n",
            "379/2465\t GPU Usage:0.8382643435800688\n",
            "380/2465\t GPU Usage:0.8382643435800688\n",
            "381/2465\t GPU Usage:0.8382643435800688\n",
            "382/2465\t GPU Usage:0.8382643435800688\n",
            "383/2465\t GPU Usage:0.8382643435800688\n",
            "384/2465\t GPU Usage:0.8382643435800688\n",
            "385/2465\t GPU Usage:0.8382643435800688\n",
            "386/2465\t GPU Usage:0.8382643435800688\n",
            "387/2465\t GPU Usage:0.8382643435800688\n",
            "388/2465\t GPU Usage:0.8382643435800688\n",
            "389/2465\t GPU Usage:0.8382643435800688\n",
            "390/2465\t GPU Usage:0.8382643435800688\n",
            "391/2465\t GPU Usage:0.8382643435800688\n",
            "392/2465\t GPU Usage:0.8382643435800688\n",
            "393/2465\t GPU Usage:0.8382643435800688\n",
            "394/2465\t GPU Usage:0.8382643435800688\n",
            "395/2465\t GPU Usage:0.8382643435800688\n",
            "396/2465\t GPU Usage:0.8382643435800688\n",
            "397/2465\t GPU Usage:0.8382643435800688\n",
            "398/2465\t GPU Usage:0.8382643435800688\n",
            "399/2465\t GPU Usage:0.8382643435800688\n",
            "400/2465\t GPU Usage:0.8382643435800688\n",
            "401/2465\t GPU Usage:0.8382643435800688\n",
            "402/2465\t GPU Usage:0.8382643435800688\n",
            "403/2465\t GPU Usage:0.8382643435800688\n",
            "404/2465\t GPU Usage:0.8382643435800688\n",
            "405/2465\t GPU Usage:0.8382643435800688\n",
            "406/2465\t GPU Usage:0.8382643435800688\n",
            "407/2465\t GPU Usage:0.8382643435800688\n",
            "408/2465\t GPU Usage:0.8382643435800688\n",
            "409/2465\t GPU Usage:0.8382643435800688\n",
            "410/2465\t GPU Usage:0.8382643435800688\n",
            "411/2465\t GPU Usage:0.8382643435800688\n",
            "412/2465\t GPU Usage:0.8382643435800688\n",
            "413/2465\t GPU Usage:0.8382643435800688\n",
            "414/2465\t GPU Usage:0.8382643435800688\n",
            "415/2465\t GPU Usage:0.8382643435800688\n",
            "416/2465\t GPU Usage:0.8382643435800688\n",
            "417/2465\t GPU Usage:0.8382643435800688\n",
            "418/2465\t GPU Usage:0.8382643435800688\n",
            "419/2465\t GPU Usage:0.8382643435800688\n",
            "420/2465\t GPU Usage:0.8382643435800688\n",
            "421/2465\t GPU Usage:0.8382643435800688\n",
            "422/2465\t GPU Usage:0.8382643435800688\n",
            "423/2465\t GPU Usage:0.8382643435800688\n",
            "424/2465\t GPU Usage:0.8382643435800688\n",
            "425/2465\t GPU Usage:0.8382643435800688\n",
            "426/2465\t GPU Usage:0.8382643435800688\n",
            "427/2465\t GPU Usage:0.8382643435800688\n",
            "428/2465\t GPU Usage:0.8382643435800688\n",
            "429/2465\t GPU Usage:0.8382643435800688\n",
            "430/2465\t GPU Usage:0.8382643435800688\n",
            "431/2465\t GPU Usage:0.8382643435800688\n",
            "432/2465\t GPU Usage:0.8382643435800688\n",
            "433/2465\t GPU Usage:0.8382643435800688\n",
            "434/2465\t GPU Usage:0.8382643435800688\n",
            "435/2465\t GPU Usage:0.8382643435800688\n",
            "436/2465\t GPU Usage:0.8382643435800688\n",
            "437/2465\t GPU Usage:0.8382643435800688\n",
            "438/2465\t GPU Usage:0.8382643435800688\n",
            "439/2465\t GPU Usage:0.8382643435800688\n",
            "440/2465\t GPU Usage:0.8382643435800688\n",
            "441/2465\t GPU Usage:0.8382643435800688\n",
            "442/2465\t GPU Usage:0.8382643435800688\n",
            "443/2465\t GPU Usage:0.8382643435800688\n",
            "444/2465\t GPU Usage:0.8382643435800688\n",
            "445/2465\t GPU Usage:0.8382643435800688\n",
            "446/2465\t GPU Usage:0.8382643435800688\n",
            "447/2465\t GPU Usage:0.8382643435800688\n",
            "448/2465\t GPU Usage:0.8382643435800688\n",
            "449/2465\t GPU Usage:0.8382643435800688\n",
            "450/2465\t GPU Usage:0.8382643435800688\n",
            "451/2465\t GPU Usage:0.8382643435800688\n",
            "452/2465\t GPU Usage:0.8382643435800688\n",
            "453/2465\t GPU Usage:0.8382643435800688\n",
            "454/2465\t GPU Usage:0.8382643435800688\n",
            "455/2465\t GPU Usage:0.8382643435800688\n",
            "456/2465\t GPU Usage:0.8382643435800688\n",
            "457/2465\t GPU Usage:0.8382643435800688\n",
            "458/2465\t GPU Usage:0.8382643435800688\n",
            "459/2465\t GPU Usage:0.8382643435800688\n",
            "460/2465\t GPU Usage:0.8382643435800688\n",
            "461/2465\t GPU Usage:0.8382643435800688\n",
            "462/2465\t GPU Usage:0.8382643435800688\n",
            "463/2465\t GPU Usage:0.8382643435800688\n",
            "464/2465\t GPU Usage:0.8382643435800688\n",
            "465/2465\t GPU Usage:0.8382643435800688\n",
            "466/2465\t GPU Usage:0.8382643435800688\n",
            "467/2465\t GPU Usage:0.8382643435800688\n",
            "468/2465\t GPU Usage:0.8382643435800688\n",
            "469/2465\t GPU Usage:0.8382643435800688\n",
            "470/2465\t GPU Usage:0.8382643435800688\n",
            "471/2465\t GPU Usage:0.8382643435800688\n",
            "472/2465\t GPU Usage:0.8382643435800688\n",
            "473/2465\t GPU Usage:0.8382643435800688\n",
            "474/2465\t GPU Usage:0.8382643435800688\n",
            "475/2465\t GPU Usage:0.8382643435800688\n",
            "476/2465\t GPU Usage:0.8382643435800688\n",
            "477/2465\t GPU Usage:0.8382643435800688\n",
            "478/2465\t GPU Usage:0.8382643435800688\n",
            "479/2465\t GPU Usage:0.8382643435800688\n",
            "480/2465\t GPU Usage:0.8382643435800688\n",
            "481/2465\t GPU Usage:0.8382643435800688\n",
            "482/2465\t GPU Usage:0.8382643435800688\n",
            "483/2465\t GPU Usage:0.8382643435800688\n",
            "484/2465\t GPU Usage:0.8382643435800688\n",
            "485/2465\t GPU Usage:0.8382643435800688\n",
            "486/2465\t GPU Usage:0.8382643435800688\n",
            "487/2465\t GPU Usage:0.8382643435800688\n",
            "488/2465\t GPU Usage:0.8382643435800688\n",
            "489/2465\t GPU Usage:0.8382643435800688\n",
            "490/2465\t GPU Usage:0.8382643435800688\n",
            "491/2465\t GPU Usage:0.8382643435800688\n",
            "492/2465\t GPU Usage:0.8382643435800688\n",
            "493/2465\t GPU Usage:0.8382643435800688\n",
            "494/2465\t GPU Usage:0.8382643435800688\n",
            "495/2465\t GPU Usage:0.8382643435800688\n",
            "496/2465\t GPU Usage:0.8382643435800688\n",
            "497/2465\t GPU Usage:0.8382643435800688\n",
            "498/2465\t GPU Usage:0.8382643435800688\n",
            "499/2465\t GPU Usage:0.8382643435800688\n",
            "500/2465\t GPU Usage:0.8382643435800688\n",
            "501/2465\t GPU Usage:0.8382643435800688\n",
            "502/2465\t GPU Usage:0.8382643435800688\n",
            "503/2465\t GPU Usage:0.8382643435800688\n",
            "504/2465\t GPU Usage:0.8382643435800688\n",
            "505/2465\t GPU Usage:0.8382643435800688\n",
            "506/2465\t GPU Usage:0.8382643435800688\n",
            "507/2465\t GPU Usage:0.8382643435800688\n",
            "508/2465\t GPU Usage:0.8382643435800688\n",
            "509/2465\t GPU Usage:0.8382643435800688\n",
            "510/2465\t GPU Usage:0.8382643435800688\n",
            "511/2465\t GPU Usage:0.8382643435800688\n",
            "512/2465\t GPU Usage:0.8382643435800688\n",
            "513/2465\t GPU Usage:0.8382643435800688\n",
            "514/2465\t GPU Usage:0.8382643435800688\n",
            "515/2465\t GPU Usage:0.8382643435800688\n",
            "516/2465\t GPU Usage:0.8382643435800688\n",
            "517/2465\t GPU Usage:0.8382643435800688\n",
            "518/2465\t GPU Usage:0.8382643435800688\n",
            "519/2465\t GPU Usage:0.8382643435800688\n",
            "520/2465\t GPU Usage:0.8382643435800688\n",
            "521/2465\t GPU Usage:0.8382643435800688\n",
            "522/2465\t GPU Usage:0.8382643435800688\n",
            "523/2465\t GPU Usage:0.8382643435800688\n",
            "524/2465\t GPU Usage:0.8382643435800688\n",
            "525/2465\t GPU Usage:0.8382643435800688\n",
            "526/2465\t GPU Usage:0.8382643435800688\n",
            "527/2465\t GPU Usage:0.8382643435800688\n",
            "528/2465\t GPU Usage:0.8382643435800688\n",
            "529/2465\t GPU Usage:0.8382643435800688\n",
            "530/2465\t GPU Usage:0.8382643435800688\n",
            "531/2465\t GPU Usage:0.8382643435800688\n",
            "532/2465\t GPU Usage:0.8382643435800688\n",
            "533/2465\t GPU Usage:0.8382643435800688\n",
            "534/2465\t GPU Usage:0.8382643435800688\n",
            "535/2465\t GPU Usage:0.8382643435800688\n",
            "536/2465\t GPU Usage:0.8382643435800688\n",
            "537/2465\t GPU Usage:0.8382643435800688\n",
            "538/2465\t GPU Usage:0.8382643435800688\n",
            "539/2465\t GPU Usage:0.8382643435800688\n",
            "540/2465\t GPU Usage:0.8382643435800688\n",
            "541/2465\t GPU Usage:0.8382643435800688\n",
            "542/2465\t GPU Usage:0.8382643435800688\n",
            "543/2465\t GPU Usage:0.8382643435800688\n",
            "544/2465\t GPU Usage:0.8382643435800688\n",
            "545/2465\t GPU Usage:0.8382643435800688\n",
            "546/2465\t GPU Usage:0.8382643435800688\n",
            "547/2465\t GPU Usage:0.8382643435800688\n",
            "548/2465\t GPU Usage:0.8382643435800688\n",
            "549/2465\t GPU Usage:0.8382643435800688\n",
            "550/2465\t GPU Usage:0.8382643435800688\n",
            "551/2465\t GPU Usage:0.8382643435800688\n",
            "552/2465\t GPU Usage:0.8382643435800688\n",
            "553/2465\t GPU Usage:0.8382643435800688\n",
            "554/2465\t GPU Usage:0.8382643435800688\n",
            "555/2465\t GPU Usage:0.8382643435800688\n",
            "556/2465\t GPU Usage:0.8382643435800688\n",
            "557/2465\t GPU Usage:0.8382643435800688\n",
            "558/2465\t GPU Usage:0.8382643435800688\n",
            "559/2465\t GPU Usage:0.8382643435800688\n",
            "560/2465\t GPU Usage:0.8382643435800688\n",
            "561/2465\t GPU Usage:0.8382643435800688\n",
            "562/2465\t GPU Usage:0.8382643435800688\n",
            "563/2465\t GPU Usage:0.8382643435800688\n",
            "564/2465\t GPU Usage:0.8382643435800688\n",
            "565/2465\t GPU Usage:0.8382643435800688\n",
            "566/2465\t GPU Usage:0.8382643435800688\n",
            "567/2465\t GPU Usage:0.8382643435800688\n",
            "568/2465\t GPU Usage:0.8382643435800688\n",
            "569/2465\t GPU Usage:0.8382643435800688\n",
            "570/2465\t GPU Usage:0.8382643435800688\n",
            "571/2465\t GPU Usage:0.8382643435800688\n",
            "572/2465\t GPU Usage:0.8382643435800688\n",
            "573/2465\t GPU Usage:0.8382643435800688\n",
            "574/2465\t GPU Usage:0.8382643435800688\n",
            "575/2465\t GPU Usage:0.8382643435800688\n",
            "576/2465\t GPU Usage:0.8382643435800688\n",
            "577/2465\t GPU Usage:0.8382643435800688\n",
            "578/2465\t GPU Usage:0.8382643435800688\n",
            "579/2465\t GPU Usage:0.8382643435800688\n",
            "580/2465\t GPU Usage:0.8382643435800688\n",
            "581/2465\t GPU Usage:0.8382643435800688\n",
            "582/2465\t GPU Usage:0.8382643435800688\n",
            "583/2465\t GPU Usage:0.8382643435800688\n",
            "584/2465\t GPU Usage:0.8382643435800688\n",
            "585/2465\t GPU Usage:0.8382643435800688\n",
            "586/2465\t GPU Usage:0.8382643435800688\n",
            "587/2465\t GPU Usage:0.8382643435800688\n",
            "588/2465\t GPU Usage:0.8382643435800688\n",
            "589/2465\t GPU Usage:0.8382643435800688\n",
            "590/2465\t GPU Usage:0.8382643435800688\n",
            "591/2465\t GPU Usage:0.8382643435800688\n",
            "592/2465\t GPU Usage:0.8382643435800688\n",
            "593/2465\t GPU Usage:0.8382643435800688\n",
            "594/2465\t GPU Usage:0.8382643435800688\n",
            "595/2465\t GPU Usage:0.8382643435800688\n",
            "596/2465\t GPU Usage:0.8382643435800688\n",
            "597/2465\t GPU Usage:0.8382643435800688\n",
            "598/2465\t GPU Usage:0.8382643435800688\n",
            "599/2465\t GPU Usage:0.8382643435800688\n",
            "600/2465\t GPU Usage:0.8382643435800688\n",
            "601/2465\t GPU Usage:0.8382643435800688\n",
            "602/2465\t GPU Usage:0.8382643435800688\n",
            "603/2465\t GPU Usage:0.8382643435800688\n",
            "604/2465\t GPU Usage:0.8382643435800688\n",
            "605/2465\t GPU Usage:0.8382643435800688\n",
            "606/2465\t GPU Usage:0.8382643435800688\n",
            "607/2465\t GPU Usage:0.8382643435800688\n",
            "608/2465\t GPU Usage:0.8382643435800688\n",
            "609/2465\t GPU Usage:0.8382643435800688\n",
            "610/2465\t GPU Usage:0.8382643435800688\n",
            "611/2465\t GPU Usage:0.8382643435800688\n",
            "612/2465\t GPU Usage:0.8382643435800688\n",
            "613/2465\t GPU Usage:0.8382643435800688\n",
            "614/2465\t GPU Usage:0.8382643435800688\n",
            "615/2465\t GPU Usage:0.8382643435800688\n",
            "616/2465\t GPU Usage:0.8382643435800688\n",
            "617/2465\t GPU Usage:0.8382643435800688\n",
            "618/2465\t GPU Usage:0.8382643435800688\n",
            "619/2465\t GPU Usage:0.8382643435800688\n",
            "620/2465\t GPU Usage:0.8382643435800688\n",
            "621/2465\t GPU Usage:0.8382643435800688\n",
            "622/2465\t GPU Usage:0.8382643435800688\n",
            "623/2465\t GPU Usage:0.8382643435800688\n",
            "624/2465\t GPU Usage:0.8382643435800688\n",
            "625/2465\t GPU Usage:0.8382643435800688\n",
            "626/2465\t GPU Usage:0.8382643435800688\n",
            "627/2465\t GPU Usage:0.8382643435800688\n",
            "628/2465\t GPU Usage:0.8382643435800688\n",
            "629/2465\t GPU Usage:0.8382643435800688\n",
            "630/2465\t GPU Usage:0.8382643435800688\n",
            "631/2465\t GPU Usage:0.8382643435800688\n",
            "632/2465\t GPU Usage:0.8382643435800688\n",
            "633/2465\t GPU Usage:0.8382643435800688\n",
            "634/2465\t GPU Usage:0.8382643435800688\n",
            "635/2465\t GPU Usage:0.8382643435800688\n",
            "636/2465\t GPU Usage:0.8382643435800688\n",
            "637/2465\t GPU Usage:0.8382643435800688\n",
            "638/2465\t GPU Usage:0.8382643435800688\n",
            "639/2465\t GPU Usage:0.8382643435800688\n",
            "640/2465\t GPU Usage:0.8382643435800688\n",
            "641/2465\t GPU Usage:0.8382643435800688\n",
            "642/2465\t GPU Usage:0.8382643435800688\n",
            "643/2465\t GPU Usage:0.8382643435800688\n",
            "644/2465\t GPU Usage:0.8382643435800688\n",
            "645/2465\t GPU Usage:0.8382643435800688\n",
            "646/2465\t GPU Usage:0.8382643435800688\n",
            "647/2465\t GPU Usage:0.8382643435800688\n",
            "648/2465\t GPU Usage:0.8382643435800688\n",
            "649/2465\t GPU Usage:0.8382643435800688\n",
            "650/2465\t GPU Usage:0.8382643435800688\n",
            "651/2465\t GPU Usage:0.8382643435800688\n",
            "652/2465\t GPU Usage:0.8382643435800688\n",
            "653/2465\t GPU Usage:0.8382643435800688\n",
            "654/2465\t GPU Usage:0.8382643435800688\n",
            "655/2465\t GPU Usage:0.8382643435800688\n",
            "656/2465\t GPU Usage:0.8382643435800688\n",
            "657/2465\t GPU Usage:0.8382643435800688\n",
            "658/2465\t GPU Usage:0.8382643435800688\n",
            "659/2465\t GPU Usage:0.8382643435800688\n",
            "660/2465\t GPU Usage:0.8382643435800688\n",
            "661/2465\t GPU Usage:0.8382643435800688\n",
            "662/2465\t GPU Usage:0.8382643435800688\n",
            "663/2465\t GPU Usage:0.8382643435800688\n",
            "664/2465\t GPU Usage:0.8382643435800688\n",
            "665/2465\t GPU Usage:0.8382643435800688\n",
            "666/2465\t GPU Usage:0.8382643435800688\n",
            "667/2465\t GPU Usage:0.8382643435800688\n",
            "668/2465\t GPU Usage:0.8382643435800688\n",
            "669/2465\t GPU Usage:0.8382643435800688\n",
            "670/2465\t GPU Usage:0.8382643435800688\n",
            "671/2465\t GPU Usage:0.8382643435800688\n",
            "672/2465\t GPU Usage:0.8382643435800688\n",
            "673/2465\t GPU Usage:0.8382643435800688\n",
            "674/2465\t GPU Usage:0.8382643435800688\n",
            "675/2465\t GPU Usage:0.8382643435800688\n",
            "676/2465\t GPU Usage:0.8382643435800688\n",
            "677/2465\t GPU Usage:0.8382643435800688\n",
            "678/2465\t GPU Usage:0.8382643435800688\n",
            "679/2465\t GPU Usage:0.8382643435800688\n",
            "680/2465\t GPU Usage:0.8382643435800688\n",
            "681/2465\t GPU Usage:0.8382643435800688\n",
            "682/2465\t GPU Usage:0.8382643435800688\n",
            "683/2465\t GPU Usage:0.8382643435800688\n",
            "684/2465\t GPU Usage:0.8382643435800688\n",
            "685/2465\t GPU Usage:0.8382643435800688\n",
            "686/2465\t GPU Usage:0.8382643435800688\n",
            "687/2465\t GPU Usage:0.8382643435800688\n",
            "688/2465\t GPU Usage:0.8382643435800688\n",
            "689/2465\t GPU Usage:0.8382643435800688\n",
            "690/2465\t GPU Usage:0.8382643435800688\n",
            "691/2465\t GPU Usage:0.8382643435800688\n",
            "692/2465\t GPU Usage:0.8382643435800688\n",
            "693/2465\t GPU Usage:0.8382643435800688\n",
            "694/2465\t GPU Usage:0.8382643435800688\n",
            "695/2465\t GPU Usage:0.8382643435800688\n",
            "696/2465\t GPU Usage:0.8382643435800688\n",
            "697/2465\t GPU Usage:0.8382643435800688\n",
            "698/2465\t GPU Usage:0.8382643435800688\n",
            "699/2465\t GPU Usage:0.8382643435800688\n",
            "700/2465\t GPU Usage:0.8382643435800688\n",
            "701/2465\t GPU Usage:0.8382643435800688\n",
            "702/2465\t GPU Usage:0.8382643435800688\n",
            "703/2465\t GPU Usage:0.8382643435800688\n",
            "704/2465\t GPU Usage:0.8382643435800688\n",
            "705/2465\t GPU Usage:0.8382643435800688\n",
            "706/2465\t GPU Usage:0.8382643435800688\n",
            "707/2465\t GPU Usage:0.8382643435800688\n",
            "708/2465\t GPU Usage:0.8382643435800688\n",
            "709/2465\t GPU Usage:0.8382643435800688\n",
            "710/2465\t GPU Usage:0.8382643435800688\n",
            "711/2465\t GPU Usage:0.8382643435800688\n",
            "712/2465\t GPU Usage:0.8382643435800688\n",
            "713/2465\t GPU Usage:0.8382643435800688\n",
            "714/2465\t GPU Usage:0.8382643435800688\n",
            "715/2465\t GPU Usage:0.8382643435800688\n",
            "716/2465\t GPU Usage:0.8382643435800688\n",
            "717/2465\t GPU Usage:0.8382643435800688\n",
            "718/2465\t GPU Usage:0.8382643435800688\n",
            "719/2465\t GPU Usage:0.8382643435800688\n",
            "720/2465\t GPU Usage:0.8382643435800688\n",
            "721/2465\t GPU Usage:0.8382643435800688\n",
            "722/2465\t GPU Usage:0.8382643435800688\n",
            "723/2465\t GPU Usage:0.8382643435800688\n",
            "724/2465\t GPU Usage:0.8382643435800688\n",
            "725/2465\t GPU Usage:0.8382643435800688\n",
            "726/2465\t GPU Usage:0.8382643435800688\n",
            "727/2465\t GPU Usage:0.8382643435800688\n",
            "728/2465\t GPU Usage:0.8382643435800688\n",
            "729/2465\t GPU Usage:0.8382643435800688\n",
            "730/2465\t GPU Usage:0.8382643435800688\n",
            "731/2465\t GPU Usage:0.8382643435800688\n",
            "732/2465\t GPU Usage:0.8382643435800688\n",
            "733/2465\t GPU Usage:0.8382643435800688\n",
            "734/2465\t GPU Usage:0.8382643435800688\n",
            "735/2465\t GPU Usage:0.8382643435800688\n",
            "736/2465\t GPU Usage:0.8382643435800688\n",
            "737/2465\t GPU Usage:0.8382643435800688\n",
            "738/2465\t GPU Usage:0.8382643435800688\n",
            "739/2465\t GPU Usage:0.8382643435800688\n",
            "740/2465\t GPU Usage:0.8382643435800688\n",
            "741/2465\t GPU Usage:0.8382643435800688\n",
            "742/2465\t GPU Usage:0.8382643435800688\n",
            "743/2465\t GPU Usage:0.8382643435800688\n",
            "744/2465\t GPU Usage:0.8382643435800688\n",
            "745/2465\t GPU Usage:0.8382643435800688\n",
            "746/2465\t GPU Usage:0.8382643435800688\n",
            "747/2465\t GPU Usage:0.8382643435800688\n",
            "748/2465\t GPU Usage:0.8382643435800688\n",
            "749/2465\t GPU Usage:0.8382643435800688\n",
            "750/2465\t GPU Usage:0.8382643435800688\n",
            "751/2465\t GPU Usage:0.8382643435800688\n",
            "752/2465\t GPU Usage:0.8382643435800688\n",
            "753/2465\t GPU Usage:0.8382643435800688\n",
            "754/2465\t GPU Usage:0.8382643435800688\n",
            "755/2465\t GPU Usage:0.8382643435800688\n",
            "756/2465\t GPU Usage:0.8382643435800688\n",
            "757/2465\t GPU Usage:0.8382643435800688\n",
            "758/2465\t GPU Usage:0.8382643435800688\n",
            "759/2465\t GPU Usage:0.8382643435800688\n",
            "760/2465\t GPU Usage:0.8382643435800688\n",
            "761/2465\t GPU Usage:0.8382643435800688\n",
            "762/2465\t GPU Usage:0.8382643435800688\n",
            "763/2465\t GPU Usage:0.8382643435800688\n",
            "764/2465\t GPU Usage:0.8382643435800688\n",
            "765/2465\t GPU Usage:0.8382643435800688\n",
            "766/2465\t GPU Usage:0.8382643435800688\n",
            "767/2465\t GPU Usage:0.8382643435800688\n",
            "768/2465\t GPU Usage:0.8382643435800688\n",
            "769/2465\t GPU Usage:0.8382643435800688\n",
            "770/2465\t GPU Usage:0.8382643435800688\n",
            "771/2465\t GPU Usage:0.8382643435800688\n"
          ]
        }
      ],
      "source": [
        "gpu_usage()\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(batched_tokens):\n",
        "        \n",
        "        inputs = batch.to(DEVICE)\n",
        "        embeddings = model(input_ids=inputs)[0]\n",
        "        output = embeddings[:, 0].cpu()\n",
        "        output = output.numpy()\n",
        "        np.savetxt(f\"./batched_embs/batch{idx}.txt\", output)\n",
        "        print(f\"{idx}/{len(batched_tokens)}\\t GPU Usage:{gpu_util()}\")\n",
        "        exit()\n",
        "        del inputs\n",
        "        del embeddings\n",
        "        del output\n",
        "        cache_clear()\n",
        "\n",
        "gpu_usage()\n",
        "gpu_util()\n",
        "cache_clear()\n",
        "gpu_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15, 768)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### If Out Of Memory error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In case of OOM unkomment all dels\n",
        "# Utilization from model alone should be ~25%\n",
        "\n",
        "del inputs\n",
        "del batched_attention_masks\n",
        "del batch \n",
        "del batched_tokens\n",
        "del embeddings\n",
        "del output\n",
        "cache_clear()\n",
        "print(torch.cuda.memory_reserved(DEVICE))\n",
        "print(torch.cuda.get_device_properties(DEVICE))\n",
        "import gc\n",
        "gc.collect()\n",
        "gpu_util()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ESM leftovers. Legacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "aIi3UZfXmpr4",
        "outputId": "a60cf137-1190-465b-f541-b5484c023007"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'esm' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load ESM-2 model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m----> 3\u001b[0m model, alphabet \u001b[39m=\u001b[39m esm\u001b[39m.\u001b[39mpretrained\u001b[39m.\u001b[39mesm2_t33_650M_UR50D()\n\u001b[0;32m      4\u001b[0m batch_converter \u001b[39m=\u001b[39m alphabet\u001b[39m.\u001b[39mget_batch_converter()\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39meval()  \u001b[39m# disables dropout for deterministic results\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'esm' is not defined"
          ]
        }
      ],
      "source": [
        "# Load ESM-2 model\n",
        "torch.cuda.empty_cache()\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "# device = \"cpu\" # My GPU doesn't have enough VRAM\n",
        "# model.to(device)\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing data and tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UUfyUL3qmtsd"
      },
      "outputs": [],
      "source": [
        "# Prepare data into format [ (label, seq), ]. We also cut * end of protein sequence symbol\n",
        "# I leave Loci_id as an unicue identificator of an entry and Gene_family as a target label\n",
        "data = list()\n",
        "for id, seq in cas_voc.iterrows():\n",
        "    data.append((f\">{seq.Gene_id}|{seq.Gene_family}|{seq.Loci_id}\", seq.Prot))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "alwcn6nrm187"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "# Takes in data in format of [ (label, seq), ] list. Applies tokens preprocessing\n",
        "# Returns: only_lables_batched, only_seqs_batched, seq_tokenized_batched \n",
        "data = data[:]\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1) # returns lengths of tokenized seqs without padding\n",
        "\n",
        "#batch_tokens = batch_tokens.to(device) # batch_converter automatically detects and moves data to gpu. But mine has too little VRAM\n",
        "#print(np.array(batch_tokens.to(\"cpu\")).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOebyg2T2W62",
        "outputId": "680e2d0a-6f00-4ea7-aa2f-a577b693db42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function Tensor.type>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_tokens.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the model and get sequence representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "g3yZNQNaw-oe",
        "outputId": "9de9eafe-9fcd-4567-96cf-d02eccf0c8fd"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-8514534e779f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtoken_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"representations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1280 dimentional (for 650M model) representations for each residue in each data entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtoken_representations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/esm/model/esm2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             x, attn = layer(\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself_attn_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/esm/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         x, attn = self.self_attn(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/esm/multihead_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;31m# don't attend to padding symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             attn_weights = attn_weights.masked_fill(\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.64 GiB (GPU 0; 14.75 GiB total capacity; 9.59 GiB already allocated; 4.06 GiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# Extract per-residue representations (on CPU)\n",
        "# Makes sence to extract only lasta layer representations. For 650M model it's layer 33\n",
        "model = model.to(device)\n",
        "batch_tokens_slice = batch_tokens.to(device)\n",
        "with torch.inference_mode():\n",
        "    results = model(batch_tokens, repr_layers=[33])\n",
        "token_representations = results[\"representations\"][33].to(\"cpu\") # 1280 dimentional (for 650M model) representations for each residue in each data entry\n",
        "token_representations.cpu()\n",
        "print(np.array(token_representations).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omsC3-6U_ICb",
        "outputId": "bfe27e54-dc1e-4043-dbc2-c258a89d4d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20, 1376, 1280)\n",
            "tensor([ 222,  103,  294, 1376,  190,  350,  325,  437, 1084,  328,  323, 1126,\n",
            "         418,  296,  339,  205,  230,  319,  243, 1125])\n",
            "(20,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-ae456d3c61a3>:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  print(np.array(sequence_representations).shape)\n",
            "<ipython-input-21-ae456d3c61a3>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  print(np.array(sequence_representations).shape)\n"
          ]
        }
      ],
      "source": [
        "# Generate per-sequence representations via averaging\n",
        "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "sequence_representations = []\n",
        "print(np.array(token_representations).shape)\n",
        "print(batch_lens)\n",
        "for i, tokens_len in enumerate(batch_lens):\n",
        "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "print(np.array(sequence_representations).shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "f775a94d50bea480a6d2d6edfa164721564b8e9539bf0cb7760835db1552d182"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
