{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sXz1K8g1Ki-1"
      },
      "source": [
        "# Embedings precomputing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependancies and mount GDrive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not running on CoLab\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Google Colab to run dumb-colab speciefic code\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    COLAB = True\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    COLAB=False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfAiyFfPUijQ",
        "outputId": "7a3f1f4c-d19d-4dcd-a655-df094a0dbd34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GPUtil in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (1.4.0)\n",
            "Requirement already satisfied: pynvml in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (11.5.0)\n",
            "Requirement already satisfied: ankh in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (1.0.0)\n",
            "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.97 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (0.1.98)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.7.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (2.11.0)\n",
            "Requirement already satisfied: biopython<2.0,>=1.80 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (1.81)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (4.28.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from biopython<2.0,>=1.80->ankh) (1.23.5)\n",
            "Requirement already satisfied: packaging in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (22.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (11.0.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (2.28.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.70.14)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.3.6)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.14.1)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (2023.4.0)\n",
            "Requirement already satisfied: xxhash in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (3.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (0.13.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (2023.3.23)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (2.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets<3.0.0,>=2.7.1->ankh) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from tqdm>=4.62.1->datasets<3.0.0,>=2.7.1->ankh) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.7.1->ankh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.7.1->ankh) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets<3.0.0,>=2.7.1->ankh) (1.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from tqdm) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# Other than torch dependencies\n",
        "# ! pip install fair-esm # Switched to Ankh embeddings\n",
        "! pip install GPUtil\n",
        "! pip install pynvml\n",
        "! pip install ankh\n",
        "! pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mWozLpHAe1Zv",
        "outputId": "a6117dd0-8dab-47b9-d551-5e0c3cacfaa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pytorch 1.13.1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Pytorch \" + torch.__version__)\n",
        "import esm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "\n",
        "import ankh\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn import metrics\n",
        "from scipy import stats\n",
        "from functools import partial\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Create device agnostic code\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hxkZ0mZ-KrSA"
      },
      "source": [
        "### Load data from TSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMLTUMt_I5FK",
        "outputId": "1a22fa46-1b8c-43a5-b4c4-ebf4628c2240"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "    # Mount GDrive for Colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Navigate Colab\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/esm\n",
        "    %ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset ~56MB\n",
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Dart-ilder/Cas_deep_search/main/esm/emb_pregen/cas_dataset_kira.tsv'\n",
        "response = requests.get(url)\n",
        "\n",
        "with open('./cas_dataset_kira.tsv', 'w') as file:\n",
        "    file.write(response.text)\n",
        "\n",
        "with open('./cas_dataset_kira.tsv') as file:\n",
        "    cas_voc = pd.read_csv(file, delimiter=\"\\t\", comment='=')\n",
        "cas_voc.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = ankh.load_base_model()\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cas_voc.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cache_clear(): # In colab to clear GPU cache you need to wait some time after deleting tensor\n",
        "    if COLAB:\n",
        "        time.sleep(0.02)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "def gpu_util(): # To monitor how much more can we load GPU with data\n",
        "    if DEVICE == \"cuda\":\n",
        "        return torch.cuda.memory_reserved(DEVICE)/torch.cuda.get_device_properties(DEVICE).total_memory\n",
        "    if DEVICE == \"cpu\":\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "protein_sequences = list(cas_voc[\"Prot\"].values)\n",
        "protein_sequences = protein_sequences[:100]\n",
        "inputs = tokenizer.batch_encode_plus(protein_sequences, \n",
        "                                        add_special_tokens=True, \n",
        "                                        padding=True,\n",
        "                                        is_split_into_words=False, \n",
        "                                        return_tensors=\"pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = inputs.to(DEVICE)\n",
        "model.to(DEVICE)\n",
        "gpu_usage()\n",
        "with torch.no_grad():\n",
        "    embeddings = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])[0]\n",
        "gpu_usage()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ESM leftovers. Legacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "aIi3UZfXmpr4",
        "outputId": "a60cf137-1190-465b-f541-b5484c023007"
      },
      "outputs": [],
      "source": [
        "# Load ESM-2 model\n",
        "torch.cuda.empty_cache()\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "# device = \"cpu\" # My GPU doesn't have enough VRAM\n",
        "# model.to(device)\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing data and tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UUfyUL3qmtsd"
      },
      "outputs": [],
      "source": [
        "# Prepare data into format [ (label, seq), ]. We also cut * end of protein sequence symbol\n",
        "# I leave Loci_id as an unicue identificator of an entry and Gene_family as a target label\n",
        "data = list()\n",
        "for id, seq in cas_voc.iterrows():\n",
        "    data.append((f\">{seq.Gene_id}|{seq.Gene_family}|{seq.Loci_id}\", seq.Prot))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "alwcn6nrm187"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "# Takes in data in format of [ (label, seq), ] list. Applies tokens preprocessing\n",
        "# Returns: only_lables_batched, only_seqs_batched, seq_tokenized_batched \n",
        "data = data[:]\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1) # returns lengths of tokenized seqs without padding\n",
        "\n",
        "#batch_tokens = batch_tokens.to(device) # batch_converter automatically detects and moves data to gpu. But mine has too little VRAM\n",
        "#print(np.array(batch_tokens.to(\"cpu\")).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOebyg2T2W62",
        "outputId": "680e2d0a-6f00-4ea7-aa2f-a577b693db42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function Tensor.type>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_tokens.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the model and get sequence representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "g3yZNQNaw-oe",
        "outputId": "9de9eafe-9fcd-4567-96cf-d02eccf0c8fd"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-8514534e779f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtoken_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"representations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1280 dimentional (for 650M model) representations for each residue in each data entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtoken_representations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/esm/model/esm2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             x, attn = layer(\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself_attn_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/esm/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         x, attn = self.self_attn(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/esm/multihead_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;31m# don't attend to padding symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             attn_weights = attn_weights.masked_fill(\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.64 GiB (GPU 0; 14.75 GiB total capacity; 9.59 GiB already allocated; 4.06 GiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# Extract per-residue representations (on CPU)\n",
        "# Makes sence to extract only lasta layer representations. For 650M model it's layer 33\n",
        "model = model.to(device)\n",
        "batch_tokens_slice = batch_tokens.to(device)\n",
        "with torch.inference_mode():\n",
        "    results = model(batch_tokens, repr_layers=[33])\n",
        "token_representations = results[\"representations\"][33].to(\"cpu\") # 1280 dimentional (for 650M model) representations for each residue in each data entry\n",
        "token_representations.cpu()\n",
        "print(np.array(token_representations).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omsC3-6U_ICb",
        "outputId": "bfe27e54-dc1e-4043-dbc2-c258a89d4d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20, 1376, 1280)\n",
            "tensor([ 222,  103,  294, 1376,  190,  350,  325,  437, 1084,  328,  323, 1126,\n",
            "         418,  296,  339,  205,  230,  319,  243, 1125])\n",
            "(20,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-ae456d3c61a3>:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  print(np.array(sequence_representations).shape)\n",
            "<ipython-input-21-ae456d3c61a3>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  print(np.array(sequence_representations).shape)\n"
          ]
        }
      ],
      "source": [
        "# Generate per-sequence representations via averaging\n",
        "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "sequence_representations = []\n",
        "print(np.array(token_representations).shape)\n",
        "print(batch_lens)\n",
        "for i, tokens_len in enumerate(batch_lens):\n",
        "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "print(np.array(sequence_representations).shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
