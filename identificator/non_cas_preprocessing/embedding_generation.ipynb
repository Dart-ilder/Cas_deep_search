{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37427,"status":"ok","timestamp":1687093227873,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"BqHLeikwRqu_","outputId":"f6919e99-cab5-415f-d5d1-64ea01b3cde5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: GPUtil in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (1.4.0)\n","Requirement already satisfied: pynvml in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (11.5.0)\n","Requirement already satisfied: ankh in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (1.0.0)\n","Requirement already satisfied: biopython<2.0,>=1.80 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (1.81)\n","Requirement already satisfied: datasets<3.0.0,>=2.7.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (2.11.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.25.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (4.28.1)\n","Requirement already satisfied: sentencepiece<0.2.0,>=0.1.97 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from ankh) (0.1.98)\n","Requirement already satisfied: numpy in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from biopython<2.0,>=1.80->ankh) (1.23.5)\n","Requirement already satisfied: pandas in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (1.5.3)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (4.65.0)\n","Requirement already satisfied: packaging in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (22.0)\n","Requirement already satisfied: aiohttp in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (3.8.4)\n","Requirement already satisfied: responses<0.19 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.18.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.3.6)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (11.0.0)\n","Requirement already satisfied: xxhash in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (3.2.0)\n","Requirement already satisfied: multiprocess in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (2023.5.0)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (0.14.1)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from datasets<3.0.0,>=2.7.1->ankh) (2.28.1)\n","Requirement already satisfied: filelock in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (3.12.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (0.13.3)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from transformers<5.0.0,>=4.25.1->ankh) (2023.3.23)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.3.3)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (2.0.4)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (22.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (1.9.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.7.1->ankh) (6.0.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets<3.0.0,>=2.7.1->ankh) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (2023.5.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.7.1->ankh) (3.4)\n","Requirement already satisfied: colorama in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from tqdm>=4.62.1->datasets<3.0.0,>=2.7.1->ankh) (0.4.6)\n","Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.7.1->ankh) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.7.1->ankh) (2022.7)\n","Requirement already satisfied: six>=1.5 in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets<3.0.0,>=2.7.1->ankh) (1.16.0)\n","Requirement already satisfied: tqdm in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (4.65.0)\n","Requirement already satisfied: colorama in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from tqdm) (0.4.6)\n","Requirement already satisfied: biopython in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (1.81)\n","Requirement already satisfied: numpy in c:\\users\\dart_ilder\\miniconda3\\envs\\.conda_cas\\lib\\site-packages (from biopython) (1.23.5)\n"]}],"source":["# Other than torch dependencies\n","! pip install GPUtil\n","! pip install pynvml\n","! pip install ankh\n","! pip install tqdm\n","! pip install biopython\n","NUMBER_OF_JUNK_PROTEINS = 1000"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13368,"status":"ok","timestamp":1687093241231,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"NUtWP20cRqvF","outputId":"38f3eab4-86c7-40de-9193-5847667e9f08"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pytorch 1.13.1\n"]}],"source":["import torch\n","print(\"Pytorch \" + torch.__version__)\n","\n","import pandas as pd\n","import numpy as np\n","import time\n","from GPUtil import showUtilization as gpu_usage\n","\n","import ankh\n","\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import Trainer, TrainingArguments, EvalPrediction\n","from datasets import load_dataset\n","\n","from sklearn import metrics\n","from scipy import stats\n","from functools import partial\n","from tqdm.auto import tqdm\n","\n","\n","# Create device agnostic code\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DEVICE"]},{"cell_type":"markdown","metadata":{"id":"-lqgQ-ZaRqvK"},"source":["### Load data from TSV\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1687093246539,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"qlMWnvvIRqvJ","outputId":"9abc42fb-46ee-459c-f65a-229fa3c02919"},"outputs":[{"name":"stdout","output_type":"stream","text":["Not running on CoLab\n"]}],"source":["# Check if running in Google Colab to run dumb-colab speciefic code\n","if 'google.colab' in str(get_ipython()):\n","    print('Running on CoLab')\n","    COLAB = True\n","else:\n","    print('Not running on CoLab')\n","    COLAB=False\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2593,"status":"ok","timestamp":1687093249776,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"321D5dcERqvN","outputId":"0f6029c0-6b77-43ff-94a8-1f9ab053aa1f"},"outputs":[],"source":["if COLAB:\n","    # Mount GDrive for Colab\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # Navigate Colab\n","    %cd /content/drive/MyDrive/Colab Notebooks/esm\n","    %ls"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1687093249777,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"0eC9fVA-RqvJ"},"outputs":[],"source":["prot_voc = pd.read_csv(f\"prot_voc_{NUMBER_OF_JUNK_PROTEINS}.tsv\", sep='\\t')"]},{"cell_type":"markdown","metadata":{"id":"XFtv2yxURqvO"},"source":["# Model download"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15655,"status":"ok","timestamp":1687093274547,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"SDnR0FqtRqvP","outputId":"7565c445-3b0e-4721-9371-d49b07f34d79"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ElnaggarLab/ankh-base were not used when initializing T5EncoderModel: ['decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight']\n","- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model, tokenizer = ankh.load_base_model()\n","model.eval()\n","with torch.no_grad():\n","  model.to(DEVICE)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1687093288052,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"v7ATSOUiRqvQ"},"outputs":[],"source":["def cache_clear(): # In colab to clear GPU cache you need to wait some time after deleting tensor\n","    if COLAB:\n","        time.sleep(0.1)\n","    torch.cuda.empty_cache()\n","\n","def gpu_util(): # To monitor how much more can we load GPU with data\n","    if DEVICE == \"cuda\":\n","        return torch.cuda.memory_reserved(DEVICE)/torch.cuda.get_device_properties(DEVICE).total_memory\n","    if DEVICE == \"cpu\":\n","        return 0\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1687093173506,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"ymW50bq9WbBj","outputId":"d7fc0969-3e1c-4cf4-9e0d-021a4494aac9"},"outputs":[{"name":"stdout","output_type":"stream","text":["| ID | GPU | MEM |\n","------------------\n","|  0 |  0% | 89% |\n"]},{"data":{"text/plain":["0"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["gpu_usage()\n","gpu_util()\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1787,"status":"ok","timestamp":1687093293185,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"xyONjKi3RqvQ","outputId":"5253c9b8-aed8-4cbe-cc4a-5112bde80f8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1000, 3133])\n"]}],"source":["protein_sequences = list(prot_voc[\"Prot\"].values)\n","tokens = tokenizer.batch_encode_plus(protein_sequences,\n","                                        add_special_tokens=True,\n","                                        padding=True,\n","                                        is_split_into_words=False,\n","                                        return_tensors=\"pt\")\n","print(tokens['input_ids'].shape)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"executionInfo":{"elapsed":962,"status":"error","timestamp":1687093303716,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"J5v2_XwyRqvR","outputId":"88531f2e-ea2f-4c38-f4db-be9fda534e81"},"outputs":[{"ename":"ValueError","evalue":"Expected a cuda device, but got: cpu","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m cache_clear()\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mmemory_reserved(DEVICE))\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_device_properties(DEVICE))\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgc\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\cuda\\memory.py:368\u001b[0m, in \u001b[0;36mmemory_reserved\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmemory_reserved\u001b[39m(device: Union[Device, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    356\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the current GPU memory managed by the caching allocator in bytes\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[39m    for a given device.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m        management.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[39mreturn\u001b[39;00m memory_stats(device\u001b[39m=\u001b[39;49mdevice)\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreserved_bytes.all.current\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\cuda\\memory.py:209\u001b[0m, in \u001b[0;36mmemory_stats\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m         result\u001b[39m.\u001b[39mappend((prefix, obj))\n\u001b[1;32m--> 209\u001b[0m stats \u001b[39m=\u001b[39m memory_stats_as_nested_dict(device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    210\u001b[0m _recurse_add_to_result(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, stats)\n\u001b[0;32m    211\u001b[0m result\u001b[39m.\u001b[39msort()\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\cuda\\memory.py:220\u001b[0m, in \u001b[0;36mmemory_stats_as_nested_dict\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_initialized():\n\u001b[0;32m    219\u001b[0m     \u001b[39mreturn\u001b[39;00m {}\n\u001b[1;32m--> 220\u001b[0m device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    221\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_memoryStats(device)\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\cuda\\_utils.py:30\u001b[0m, in \u001b[0;36m_get_device_index\u001b[1;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpected a cuda or cpu device, but got: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(device))\n\u001b[0;32m     29\u001b[0m     \u001b[39melif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mExpected a cuda device, but got: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(device))\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[0;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice):\n","\u001b[1;31mValueError\u001b[0m: Expected a cuda device, but got: cpu"]}],"source":["cache_clear()\n","print(torch.cuda.memory_reserved(DEVICE))\n","print(torch.cuda.get_device_properties(DEVICE))\n","import gc\n","gc.collect()\n","gpu_util()\n","gpu_usage()"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":418,"status":"ok","timestamp":1687093326502,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"GvAhz66vRqvS","outputId":"186d9e75-0f59-4d0f-e18c-6f20ad4fc78b"},"outputs":[{"data":{"text/plain":["torch.Size([1, 3133])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["BATCH_SIZE = 8 # For A100 Collab\n","batched_tokens = torch.split(tokens['input_ids'], BATCH_SIZE)\n","batched_attention_masks = torch.split(tokens['attention_mask'], BATCH_SIZE)\n","batched_tokens[0].shape"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1687093327976,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"mNeyXTvCaeUW","outputId":"2bb407f1-6c90-45f2-b24c-bc427cbccf5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["72 \t 25064048\n","64\n","25064\n"]}],"source":["import sys\n","print(sys.getsizeof(batched_tokens[0]), \"\\t\", sys.getsizeof(batched_tokens[0].storage()))\n","print(sys.getsizeof(tokens['input_ids'].storage))\n","print(batched_tokens[0].element_size() * batched_tokens[0].nelement())"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["%mkdir batched_embs"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8RCkD6wRqvS","outputId":"c28cd838-4f1f-4385-b462-17c2719386f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["| ID | GPU | MEM |\n","------------------\n","|  0 |  0% | 89% |\n","0/1000\t GPU Usage:0\n","1/1000\t GPU Usage:0\n","2/1000\t GPU Usage:0\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batched_tokens):\n\u001b[0;32m      4\u001b[0m     inputs \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m----> 5\u001b[0m     embeddings \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minputs)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m     output \u001b[39m=\u001b[39m embeddings[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m      8\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mnumpy()\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1926\u001b[0m, in \u001b[0;36mT5EncoderModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1908\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1909\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1922\u001b[0m \u001b[39m>>> last_hidden_states = outputs.last_hidden_state\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1926\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1927\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1928\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1929\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1930\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1931\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1932\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1933\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1934\u001b[0m )\n\u001b[0;32m   1936\u001b[0m \u001b[39mreturn\u001b[39;00m encoder_outputs\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1086\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1073\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1074\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1075\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     )\n\u001b[0;32m   1085\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1086\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1087\u001b[0m         hidden_states,\n\u001b[0;32m   1088\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1089\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   1090\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1091\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1092\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   1093\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   1094\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   1095\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1096\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1097\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1098\u001b[0m     )\n\u001b[0;32m   1100\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:693\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 693\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[0;32m    694\u001b[0m     hidden_states,\n\u001b[0;32m    695\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    696\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    697\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    698\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    699\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    700\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    701\u001b[0m )\n\u001b[0;32m    702\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    703\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:600\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    591\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    597\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m ):\n\u001b[0;32m    599\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 600\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[0;32m    601\u001b[0m         normed_hidden_states,\n\u001b[0;32m    602\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    603\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    604\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    605\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    606\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    607\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m    610\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:519\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n\u001b[0;32m    518\u001b[0m \u001b[39m# get query states\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m query_states \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq(hidden_states))  \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[39m# get key/value states\u001b[39;00m\n\u001b[0;32m    522\u001b[0m key_states \u001b[39m=\u001b[39m project(\n\u001b[0;32m    523\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk, key_value_states, past_key_value[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    524\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Dart_ilder\\miniconda3\\envs\\.conda_Cas\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["gpu_usage()\n","with torch.no_grad():\n","    for idx, batch in enumerate(batched_tokens):\n","        inputs = batch.to(DEVICE)\n","        embeddings = model(input_ids=inputs)[0] # Most restricting moment. Model returns output with attention maps that clog the GPU Memory.\n","        output = embeddings[:, 0].cpu()\n","        output = output.numpy()\n","        np.savetxt(f\"./batched_embs/batch{idx}.txt\", output)\n","        print(f\"{idx}/{len(batched_tokens)}\\t GPU Usage:{gpu_util()}\")\n","        del inputs\n","        del embeddings\n","        del output\n","        cache_clear()\n","        #exit()\n","\n","gpu_usage()\n","gpu_util()\n","cache_clear()\n","gpu_usage()"]},{"cell_type":"markdown","metadata":{"id":"NRPv5rs0RqvT"},"source":["### If Out Of Memory error:"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1687090626438,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"p2n7ntrSRqvT","outputId":"04da42ea-81c5-4e8e-a8b9-9a44bec761bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["3204448256\n","_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n"]},{"data":{"text/plain":["0.20235981608167894"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# In case of OOM\n","# Utilization from model alone should be ~25% (for GTX 1080Ti)\n","try:\n","  del inputs\n","except:\n","  pass\n","try:\n","  del batched_attention_masks\n","except:\n","    pass\n","try:\n","  del batch\n","except:\n","    pass\n","try:\n","  del batched_tokens\n","except:\n","    pass\n","try:\n","  del embeddings\n","except:\n","    pass\n","try:\n","  del output\n","except:\n","  pass\n","cache_clear()\n","print(torch.cuda.memory_reserved(DEVICE))\n","print(torch.cuda.get_device_properties(DEVICE))\n","import gc\n","gc.collect()\n","gpu_util()"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687090627005,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"3vp0L3RCVDdE","outputId":"ce204abe-908a-43c7-b111-c7fc62a9dca9"},"outputs":[{"name":"stdout","output_type":"stream","text":["3204448256\n","_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n","| ID | GPU | MEM |\n","------------------\n","|  0 |  0% | 27% |\n"]},{"data":{"text/plain":["0.20235981608167894"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# Clear model itself from GPU - last resort\n","try:\n","  del model\n","except:\n","  pass\n","cache_clear()\n","print(torch.cuda.memory_reserved(DEVICE))\n","print(torch.cuda.get_device_properties(DEVICE))\n","import gc\n","gpu_usage()\n","gc.collect()\n","gpu_util()"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11724,"status":"ok","timestamp":1687090642019,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"0a7oH3z2Xnn2","outputId":"0936822d-3a4e-49b3-eabc-5919eda0d5ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial GPU Usage\n","| ID | GPU | MEM |\n","------------------\n","|  0 |  0% | 27% |\n","GPU Usage after allcoating a bunch of Tensors\n","| ID | GPU | MEM |\n","------------------\n","|  0 |  2% | 34% |\n","GPU Usage after deleting the Tensors\n","| ID | GPU | MEM |\n","------------------\n","|  0 |  2% | 34% |\n","GPU Usage after emptying the cache\n","| ID | GPU | MEM |\n","------------------\n","|  0 | 15% |  9% |\n"]}],"source":["# Test of GPU cache clearing\n","\n","import torch\n","from GPUtil import showUtilization as gpu_usage\n","\n","print(\"Initial GPU Usage\")\n","gpu_usage()\n","\n","tensorList = []\n","for x in range(10):\n","  tensorList.append(torch.randn(10000000,10).cuda())   # reduce the size of tensor if you are getting OOM\n","\n","\n","\n","print(\"GPU Usage after allcoating a bunch of Tensors\")\n","gpu_usage()\n","\n","del tensorList\n","\n","print(\"GPU Usage after deleting the Tensors\")\n","gpu_usage()\n","\n","print(\"GPU Usage after emptying the cache\")\n","cache_clear()\n","gpu_usage()"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1687090056508,"user":{"displayName":"Илья Шаров","userId":"17851219735905790936"},"user_tz":-180},"id":"LR2ljKerXn_H"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
